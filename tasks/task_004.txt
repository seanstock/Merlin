# Task ID: 4
# Title: OpenAI Integration and API Client
# Status: done
# Dependencies: 1, 3
# Priority: high
# Description: Implement the OpenAI API client with function-calling capabilities to power the AI Tutor feature, maintaining context of past interactions.
# Details:
1. Create OpenAI API client wrapper:
```kotlin
class OpenAIClient(private val apiKey: String) {
    private val service = OpenAI(apiKey)
    
    suspend fun getAIResponse(
        childProfile: ChildProfile,
        chatHistory: List<ChatMessage>,
        functions: List<FunctionDefinition>
    ): AIResponse {
        val systemPrompt = "You are Merlin, a friendly AI tutor for a ${childProfile.age}-year-old ${childProfile.gender} child located in ${childProfile.location}, speaking ${childProfile.preferredLanguage}. Your role is to offer engaging educational tasks or launch games, provide supportive feedback, and adjust task difficulty dynamically to achieve ~80% success rate."
        
        // Implementation using OpenAI SDK
    }
}
```

2. Implement function calling for game launching:
```kotlin
val launchGameFunction = FunctionDefinition(
    name = "launch_game",
    description = "Launch an educational game for the child",
    parameters = mapOf(
        "game_id" to Parameter(type = "string", description = "ID of the game to launch"),
        "level" to Parameter(type = "integer", description = "Difficulty level of the game")
    )
)
```

3. Create context management to maintain rolling window of last 20 interactions
4. Implement error handling and retry logic for API failures
5. Add caching mechanism to reduce API calls

# Test Strategy:
Create mock OpenAI responses to test function calling and context management. Verify error handling by simulating network failures. Test context window maintenance by creating long conversation sequences.

# Subtasks:
## 1. API Client Wrapper Implementation with Authentication [done]
### Dependencies: None
### Description: Develop a reusable API client wrapper for OpenAI, ensuring secure API key handling and proper authentication headers.
### Details:
Implementation: Create a class or module that encapsulates all OpenAI API interactions. Load the API key securely from environment variables or a key management service, never hardcoding it or exposing it in client-side code. Attach the API key as a Bearer token in the Authorization header for all requests. Support configuration for organization/project headers if needed.
Testing Approach: Write unit tests to verify that the API key is loaded securely, headers are set correctly, and requests are properly authenticated. Use mock servers to ensure no real API calls are made during tests.
Acceptance Criteria: The wrapper must securely load the API key, attach correct headers, and successfully authenticate with the OpenAI API. No API key should be exposed in logs or client-side code.
<info added on 2025-05-27T22:46:02.851Z>
# OpenAI API Client Wrapper Implementation Plan

## Architecture
- Create `OpenAIClientWrapper` class in `Merlin/data/src/main/java/com/example/merlin/data/remote/`
- Implement as a singleton or use dependency injection to provide instance where needed
- Encapsulate all OpenAI API interactions within this wrapper

## Dependencies
- Add OpenAI SDK to `Merlin/data/build.gradle.kts`:
  ```kotlin
  implementation("com.aallam.openai:openai-client:LATEST_VERSION")
  ```
- Ensure internet permission in app manifest:
  ```xml
  <uses-permission android:name="android.permission.INTERNET" />
  ```

## API Key Management
- Store API key in `Merlin/local.properties` (gitignored):
  ```
  OPENAI_API_KEY="sk-YOUR_ACTUAL_OPENAI_API_KEY"
  ```
- Configure build.gradle.kts to read from local.properties:
  ```kotlin
  val localProperties = java.util.Properties()
  val localPropertiesFile = rootProject.file("local.properties")
  if (localPropertiesFile.exists()) {
      localPropertiesFile.inputStream().use { localProperties.load(it) }
  }
  val openAIApiKey = localProperties.getProperty("OPENAI_API_KEY", "YOUR_API_KEY_NOT_FOUND_PLACEHOLDER")

  android {
      buildFeatures {
          buildConfig = true
      }
      buildTypes {
          debug {
              buildConfigField("String", "OPENAI_API_KEY", "\"$openAIApiKey\"")
          }
          release {
              buildConfigField("String", "OPENAI_API_KEY", "\"$openAIApiKey\"")
          }
      }
  }
  ```

## Implementation Details
- Create wrapper class with secure initialization:
  ```kotlin
  class OpenAIClientWrapper {
      private val apiKey: String = BuildConfig.OPENAI_API_KEY
      private val openAI: OpenAI

      init {
          if (apiKey == "YOUR_API_KEY_NOT_FOUND_PLACEHOLDER" || apiKey.isBlank()) {
              throw IllegalArgumentException("OpenAI API key not found. Please set it in local.properties.")
          }
          openAI = OpenAI(token = apiKey)
      }
      
      // Methods for API interactions will be added in subsequent tasks
  }
  ```

## Security Considerations
- Never hardcode the API key in source code
- Ensure API key is not logged or exposed in client-side code
- For production, consider more secure key management solutions

## Testing Approach
- Unit test API key loading mechanism
- Verify error handling for missing/invalid API keys
- Use mock server to test authentication without making real API calls
</info added on 2025-05-27T22:46:02.851Z>
<info added on 2025-05-27T22:47:35.089Z>
# OpenAI API Client Wrapper Implementation Plan

## Architecture
- Create `OpenAIClientWrapper` class in `Merlin/data/src/main/java/com/example/merlin/data/remote/`
- Implement as a singleton or use dependency injection to provide instance where needed
- Encapsulate all OpenAI API interactions within this wrapper

## Dependencies
- Add OpenAI SDK to `Merlin/data/build.gradle.kts`:
  ```kotlin
  implementation("com.aallam.openai:openai-client:LATEST_VERSION")
  ```
- Ensure internet permission in app manifest:
  ```xml
  <uses-permission android:name="android.permission.INTERNET" />
  ```

## API Key Management
- Store API key in `Merlin/local.properties` (gitignored):
  ```
  OPENAI_API_KEY="sk-YOUR_ACTUAL_OPENAI_API_KEY"
  ```
- Configure build.gradle.kts to read from local.properties:
  ```kotlin
  val localProperties = java.util.Properties()
  val localPropertiesFile = rootProject.file("local.properties")
  if (localPropertiesFile.exists()) {
      localPropertiesFile.inputStream().use { localProperties.load(it) }
  }
  val openAIApiKey = localProperties.getProperty("OPENAI_API_KEY", "KEY_NOT_FOUND_IN_LOCAL_PROPERTIES")

  android {
      buildFeatures {
          buildConfig = true
      }
      buildTypes {
          debug {
              buildConfigField("String", "OPENAI_API_KEY", "\"$openAIApiKey\"")
          }
          release {
              buildConfigField("String", "OPENAI_API_KEY", "\"$openAIApiKey\"")
          }
      }
  }
  ```

## Implementation Details
- Create wrapper class with secure initialization:
  ```kotlin
  package com.example.merlin.data.remote

  import com.aallam.openai.client.OpenAI
  import com.example.merlin.data.BuildConfig
  import android.util.Log

  class OpenAIClientWrapper {
      private val apiKey: String = BuildConfig.OPENAI_API_KEY
      private var openAI: OpenAI? = null

      init {
          if (apiKey == "KEY_NOT_FOUND_IN_LOCAL_PROPERTIES" || apiKey.isBlank() || !apiKey.startsWith("sk-")) {
              Log.e("OpenAIClientWrapper", "OpenAI API key is invalid or not found. Please set a valid OPENAI_API_KEY in local.properties.")
          } else {
              openAI = OpenAI(token = apiKey)
              Log.d("OpenAIClientWrapper", "OpenAI client initialized successfully.")
          }
      }

      fun isInitialized(): Boolean = openAI != null

      // Placeholder for chat completion method - to be implemented in subsequent tasks
  }
  ```

## Security Considerations
- Never hardcode the API key in source code
- Ensure API key is not logged or exposed in client-side code
- For production, consider more secure key management solutions

## Testing Approach
- Unit test API key loading mechanism
- Verify error handling for missing/invalid API keys
- Use mock server to test authentication without making real API calls

## Implementation Sequence
1. Verify/update OpenAI SDK dependency in `Merlin/data/build.gradle.kts`
2. Ensure `OPENAI_API_KEY` is in `Merlin/local.properties`
3. Modify `Merlin/data/build.gradle.kts` to add `buildFeatures { buildConfig = true }` and the `buildConfigField` definitions
4. Create `OpenAIClientWrapper.kt` in the `com.example.merlin.data.remote` package
5. Verify Internet permission in `Merlin/app/src/main/AndroidManifest.xml`
6. Sync Gradle and build
</info added on 2025-05-27T22:47:35.089Z>

## 2. Function Calling Mechanism for Game Launching [done]
### Dependencies: 4.1
### Description: Implement a mechanism to call OpenAI functions for launching games, including defining necessary data structures.
### Details:
Implementation: Define data structures such as ChatMessage, FunctionDefinition, Parameter, and AIResponse to model the request and response formats. Implement a method to send function call requests to OpenAI, passing the appropriate payload and handling the function_call field in the response.
Testing Approach: Write unit and integration tests to ensure correct serialization/deserialization of data structures and that function calls are correctly formed and parsed. Use mock responses to simulate OpenAI API behavior.
Acceptance Criteria: The system must correctly construct and send function call requests, parse responses, and handle all defined data structures without errors.
<info added on 2025-05-27T22:50:10.436Z>
# Function Calling Implementation Plan

## Data Structures
- Create package `com.example.merlin.data.model.openaidl` for custom abstractions
- Leverage SDK classes where possible:
  - Use `com.aallam.openai.api.chat.ChatMessage` and `ChatRole` from SDK
  - Use `com.aallam.openai.api.chat.FunctionTool` and `FunctionDefinition` from SDK
- Create helper data class `FunctionParameterProperty`:
  ```kotlin
  data class FunctionParameterProperty(
      val type: String, 
      val description: String, 
      val enum: List<String>? = null
  )
  ```
- Create response wrapper `MerlinAIResponse`:
  ```kotlin
  data class MerlinAIResponse(
      val content: String?, // Assistant's textual response
      val functionCallName: String?,
      val functionCallArguments: String? // Raw JSON string of arguments
  )
  ```

## OpenAIClientWrapper Modifications
- Add helper function `buildParametersJson` to convert user-friendly Kotlin structures to required JsonObject format
- Implement main method:
  ```kotlin
  suspend fun getChatCompletionWithFunctions(
      chatMessages: List<ChatMessageInfo>,
      functionTools: List<FunctionTool>
  ): MerlinAIResponse?
  ```
- Create utility function for game launching:
  ```kotlin
  fun createLaunchGameFunctionTool(): FunctionTool
  ```

## Implementation Steps
1. Create required data classes and package structure
2. Implement parameter JSON builder helper
3. Add function calling capability to OpenAIClientWrapper
4. Create predefined function tools (starting with game launcher)
5. Write unit tests with mock OpenAI client

## Technical Details
- Parameters must follow OpenAI's JSON Schema format with proper "type", "properties", and optional "required" fields
- Handle error cases gracefully with appropriate logging
- Ensure proper null handling for optional fields
</info added on 2025-05-27T22:50:10.436Z>

## 3. Conversation Context Management with Rolling Window [done]
### Dependencies: 4.2
### Description: Implement a rolling window mechanism to maintain the last N (e.g., 20) interactions in conversation context.
### Details:
Implementation: Create a ChatHistory manager that stores conversation turns as ChatMessage objects. Ensure only the most recent N interactions are retained, discarding older ones as new messages arrive. Integrate this context into API requests.
Testing Approach: Write unit tests to verify that the rolling window correctly maintains the last N messages and discards older ones. Test edge cases such as adding fewer than N messages and exceeding the window size.
Acceptance Criteria: The context manager must always provide the last N interactions for API calls, with correct ordering and no memory leaks.
<info added on 2025-05-27T22:52:32.646Z>
# Implementation Details

## ConversationContextManager Class
- Create a new package `com.example.merlin.data.manager` in the data module
- Implement `ConversationContextManager.kt` using LinkedList to maintain a rolling window of conversation history
- Support a configurable maximum size (default: 20 messages)
- Include special handling for system prompts to ensure they remain at the beginning of the context
- Implement methods:
  - `initializeWithSystemPrompt(systemPromptContent: String)` - Sets up a new conversation with a system prompt
  - `addMessage(message: ChatMessageInfo)` - Adds a message while maintaining the rolling window
  - `addUserMessage(content: String)` - Convenience method for adding user messages
  - `addAssistantMessage(content: String?, toolCalls: List<ToolCall>?)` - Convenience method for assistant responses
  - `addToolMessage(toolCallId: String, content: String)` - For adding tool execution results
  - `getFormattedHistory(): List<ChatMessageInfo>` - Returns a copy of the current conversation history
  - `clearHistory()` - Resets the conversation context

## Integration Strategy
- The ConversationContextManager will be instantiated by higher-level components (ViewModels or UseCases)
- When making API calls, pass `conversationContextManager.getFormattedHistory()` to include conversation context
- System prompts will be initialized at the start of new conversations

## Testing Strategy
- Unit test the ConversationContextManager class to verify:
  - Correct message addition behavior
  - Rolling window functionality (oldest messages removed when exceeding maxSize)
  - System prompt preservation
  - History clearing functionality
  - Accurate history retrieval
</info added on 2025-05-27T22:52:32.646Z>

## 4. Error Handling and Retry Logic for API Calls [done]
### Dependencies: 4.1
### Description: Implement robust error handling and retry logic for OpenAI API calls to handle transient failures.
### Details:
Implementation: Add error handling to the API client wrapper to catch network errors, rate limits, and server errors. Implement an exponential backoff retry mechanism for transient errors, with a configurable maximum number of retries.
Testing Approach: Use unit tests with mocked API responses to simulate various error conditions (timeouts, 429, 500, etc.) and verify that retries occur as expected. Ensure that unrecoverable errors are surfaced appropriately.
Acceptance Criteria: The client must retry transient errors up to the configured limit and handle all error types gracefully, providing clear error messages for unrecoverable failures.
<info added on 2025-05-27T22:54:39.647Z>
# Error Handling and Retry Logic Implementation Plan

## Configuration Constants
- `MAX_RETRIES = 3`
- `INITIAL_BACKOFF_DELAY_MS = 1000L`
- `MAX_BACKOFF_DELAY_MS = 16000L`
- `BACKOFF_MULTIPLIER = 2.0`

## Implementation Structure
1. Modify `OpenAIClientWrapper.getChatCompletionWithFunctions` to include:
   - Retry counter and backoff delay tracking
   - While loop for retry attempts
   - Exponential backoff calculation
   - Proper logging at each attempt

2. Exception handling hierarchy:
   - `OpenAIAPIException`: Handle API-specific errors
   - `OpenAIHttpException`: Handle HTTP transport errors
   - `IOException`: Handle network connectivity issues
   - General `Exception`: Catch unexpected errors

3. Helper method `isRetryableHttpError(statusCode: Int?)` to determine:
   - Retryable status codes: 408, 429, 500, 502, 503, 504
   - Non-retryable errors should fail immediately

## Error Response Handling
- Return appropriate `MerlinAIResponse` with error details
- Include status codes and error messages in the response
- Ensure client code can distinguish between different error types

## Testing Strategy
- Mock the OpenAI client to simulate various error conditions
- Test retry behavior with 429 (rate limit) errors
- Test retry behavior with 500-series errors
- Test immediate failure with non-retryable errors
- Verify correct backoff timing between retries
- Confirm max retry limit is respected
- Validate error information is properly propagated

## Implementation Steps
1. Add retry configuration constants to `OpenAIClientWrapper.kt`
2. Implement the retry helper function
3. Refactor the API call method with retry loop
4. Add comprehensive logging
5. Create unit tests for verification
</info added on 2025-05-27T22:54:39.647Z>

## 5. Response Caching System [done]
### Dependencies: 4.1, 4.2, 4.3, 4.4
### Description: Implement a basic response caching system (in-memory or file-based) for identical prompts within a short timeframe.
### Details:
Implementation: Develop a cache layer that stores responses keyed by a hash of the prompt and context. Use an in-memory cache (e.g., LRU) or a simple file-based cache with expiration logic. Integrate the cache into the API client so that repeated identical requests within a set timeframe return cached responses.
Testing Approach: Write unit tests to verify that identical requests return cached responses, cache entries expire as expected, and cache does not serve stale data. Test concurrency if applicable.
Acceptance Criteria: The caching system must return cached responses for identical prompts within the cache window, expire entries correctly, and not interfere with normal API operation.
<info added on 2025-05-27T22:56:35.118Z>
# Implementation Plan for Response Caching System

## Cache Design
- Implement an in-memory LRU (Least Recently Used) cache in the OpenAIClientWrapper class
- Cache size: 10 responses maximum
- Cache key: Hash of prompt content and function tools
- Cache value: Complete MerlinAIResponse objects

## Implementation Steps
1. Add dependency for androidx.collection:collection-ktx to access LruCache
2. Define cache constants in OpenAIClientWrapper.kt:
   - RESPONSE_CACHE_SIZE = 10
3. Initialize LruCache as a member variable in OpenAIClientWrapper
4. Create a generateCacheKey() helper function that:
   - Concatenates message content with role information
   - Includes function tool names if present
   - Returns a deterministic string key
5. Modify getChatCompletionWithFunctions() to:
   - Check cache before making API calls
   - Store successful responses in cache
   - Skip caching for error responses
   - Log cache hits and misses

## Cache Invalidation Strategy
- Rely on LruCache's built-in size-based eviction
- No time-based expiration in initial implementation
- Consider adding timestamp-based expiration in future iterations

## Testing Strategy
- Write unit tests for OpenAIClientWrapper caching logic
- Test scenarios:
  - Cache hit: Verify identical requests use cached response
  - Cache miss: Verify different requests trigger API calls
  - Cache eviction: Verify older items are removed when cache exceeds size limit
- Mock OpenAI client to isolate cache testing
</info added on 2025-05-27T22:56:35.118Z>

## 6. Create AI Service Interface Abstraction [done]
### Dependencies: None
### Description: Create provider-agnostic AI service interface to abstract OpenAI dependencies for Learning-as-a-Service architecture
### Details:
Create clean interface boundary between AI providers and application logic. This enables switching between OpenAI, Claude, Gemini, or remote AI services without breaking changes.

## 7. Create OpenAI Service Implementation [done]
### Dependencies: 4.6
### Description: Wrap existing OpenAIClientWrapper into new AIService interface without breaking existing functionality
### Details:
Create OpenAIService class that implements AIService interface and wraps OpenAIClientWrapper. Add conversion utilities between domain types (AIMessage, AIFunctionTool) and OpenAI SDK types. Test compatibility to ensure no breaking changes.
<info added on 2025-05-30T23:49:58.469Z>
Implementation complete. Created OpenAIService class that successfully implements the AIService interface while wrapping the existing OpenAIClientWrapper. Added AIServiceProvider to supply the OpenAI implementation by default. The implementation follows a bridge pattern approach, maintaining clean module separation by keeping OpenAI SDK dependencies isolated in the data module. Used stub implementation to avoid cross-module SDK dependencies, with proper conversion logic planned for the next task. All existing functionality remains intact with no breaking changes, and the project builds successfully. The architecture achieves clean separation of concerns while providing a stable interface boundary between the application and data layers.
</info added on 2025-05-30T23:49:58.469Z>

## 8. Update AIInteractionManager to use AIService [done]
### Dependencies: 4.7
### Description: Switch core AI orchestration layer to use AIService interface instead of direct OpenAIClientWrapper
### Details:
Modify AIInteractionManager to inject AIService dependency and use domain types. Update conversation context management and function calling. Test that conversations still work correctly after migration.
<info added on 2025-05-30T23:56:16.748Z>
AIInteractionManager has been successfully updated to use the AIService interface instead of directly depending on OpenAIClientWrapper. The implementation follows the adapter pattern, with AIServiceAdapter serving as the bridge between the interface and OpenAI implementation. All AI calls now flow through the service interface boundary while maintaining existing functionality.

Key achievements:
- Modified AIInteractionManager.kt to inject AIServiceInterface dependency
- Updated ChatViewModel.kt to use OpenAIServiceAdapter for dependency injection
- Implemented AIServiceAdapter.kt as a working bridge
- Preserved all existing types (ChatMessage, FunctionTool, MerlinAIResponse) to avoid module dependency issues
- Maintained conversation context management and function calling capabilities
- Verified that all conversations work correctly after migration
- Ensured clean dependency injection with proper instantiation in ChatViewModel

The architecture now supports future remote AI service implementations while the current implementation successfully passed testing for the entire AI conversation flow.
</info added on 2025-05-30T23:56:16.748Z>

## 9. Update MemorySummarizer to use AIService [done]
### Dependencies: 4.8
### Description: Switch memory processing component to use AIService interface instead of direct OpenAI dependencies
### Details:
Update MemorySummarizer to use AIService for memory processing operations. Convert to use domain types and test that memory functionality works correctly.
<info added on 2025-05-30T23:57:23.138Z>
Successfully migrated MemorySummarizer to use AIServiceInterface instead of direct OpenAIClientWrapper dependency. Updated constructor to accept AIServiceInterface parameter and modified AIInteractionManager to pass its shared aiService instance to MemorySummarizer. This change eliminates duplicate OpenAI client creation, improves resource efficiency, and maintains clean architecture by ensuring all AI calls flow through a single service interface boundary. All existing memory processing logic and request patterns were preserved, with zero breaking changes to functionality. Testing confirms that the complete AI conversation flow including memory processing works correctly. The migration achieves unified AI service usage across the application while maintaining all memory summarization capabilities.
</info added on 2025-05-30T23:57:23.138Z>

## 10. Complete AI Service Migration [done]
### Dependencies: 4.9
### Description: Update ChatViewModel, complete dependency injection setup, and remove direct OpenAI dependencies from app module
### Details:
Final migration step: Update ChatViewModel and any remaining components to use AIService. Complete AIServiceProvider implementation with proper dependency injection. Remove direct OpenAI imports from app module. Verify entire AI system works through clean interface.
<info added on 2025-05-31T00:00:01.346Z>
✅ Task 5 Complete: Complete AI Service Migration

Successfully completed the final AI service migration and dependency cleanup:

COMPLETED WORK:
• ChatViewModel.kt - Updated to use OpenAIServiceAdapter directly from data module
• AIServiceProvider.kt - Simplified to avoid circular dependencies, ready for future remote implementations
• Removed stub OpenAIService.kt from app module - eliminated redundant code
• Cleaned up all OpenAI SDK imports from app module
• Build successful - All AI functionality working through clean service interface

ARCHITECTURE ACHIEVEMENTS:
✅ Complete AI service abstraction - All AI calls flow through AIServiceInterface
✅ Clean module boundaries - App module no longer directly depends on OpenAI SDK types  
✅ Zero breaking changes - Full AI conversation, memory, and function calling preserved
✅ Simplified dependencies - Direct use of data module's proven OpenAIServiceAdapter
✅ Future-ready architecture - Clean foundation for remote AI service implementations

IMPLEMENTATION APPROACH:
- Removed OpenAI SDK imports from app module while maintaining functionality
- Used data module's OpenAIServiceAdapter directly to avoid circular dependencies
- Simplified AIServiceProvider for future dependency injection scenarios  
- Maintained all existing AI conversation flows, memory processing, and function calling
- Successfully tested complete AI system through clean interface boundary

FINAL RESULT:
✅ Learning-as-a-Service AI architecture complete
✅ 4-6 hour refactoring scope achieved successfully  
✅ Clean service interface boundary established
✅ Ready for local/remote AI service implementations
✅ All original functionality preserved and working
</info added on 2025-05-31T00:00:01.346Z>

